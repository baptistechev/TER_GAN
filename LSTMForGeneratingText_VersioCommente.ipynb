{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMForGeneratingText-VersioCommente.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePKb-MvOb4HR",
        "outputId": "e55fc90c-6a15-4ebf-f479-2366576edc31"
      },
      "source": [
        "# pour monter son drive Google Drive local\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My Drive/Colab Notebooks\n",
        "%ls\n",
        "# pour une utilisation sur une machine locale changer le chemin ci-dessous et décommenter la ligne\n",
        "#%cd ./"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/Colab Notebooks\n",
            "'1_-_Classification_de_donnees (1).ipynb'\n",
            " 1_-_Classification_de_donnees.ipynb\n",
            " 2_-_Ingenierie_des_donnees_textuelles.ipynb\n",
            " 3_-_Classification_de_donnees_textuelles.ipynb\n",
            " 6_-_Les_word_embeddings_solutionsintegrees.ipynb\n",
            "'Copie de Untitled0.ipynb'\n",
            " firstmodel.pkl\n",
            " glove.6B.100d.txt\n",
            " glove.6B.100d.w2vformat.txt\n",
            " glove.6B.50d.txt\n",
            " glove.6B.50d.w2vformat.txt\n",
            " IrisGAN.ipynb\n",
            " LSTMForGeneratingText.ipynb\n",
            " LSTMForGeneratingText-VersioCommente.ipynb\n",
            " model_alice.bin\n",
            " model.png\n",
            " model_reviews.bin\n",
            " MyNLPUtilities.py\n",
            " pkl_modelNB.sav\n",
            " \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
            " ReviewsLabelled.csv\n",
            " weights-improvement-50-1.2016-bigger.hdf5\n",
            " wonderland.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05aWFL9ld8NW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187b8d1b-dc1c-4070-9c99-04c51a697d19"
      },
      "source": [
        "#https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
        "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "\n",
        "#charge fichier\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "\n",
        "### pre-traitement du texte ###\n",
        "\n",
        "raw_text = raw_text.lower() #texte en minuscule\n",
        "chars = sorted(list(set(raw_text))) # creation dictionnaire\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) # code du dico en entiers\n",
        "\n",
        "n_chars = len(raw_text) #nb caractere du texte\n",
        "n_vocab = len(chars) #nb de lettres distincte\n",
        "# print (\"Total Characters: \", n_chars)\n",
        "# print (\"Total Vocab: \", n_vocab)\n",
        "\n",
        "### Creation des matrices entré / sortie ###\n",
        "seq_length = 100 #100 pour Alice\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "  # print (n_chars - seq_length,' ',i,\"raw_text[i:i + seq_length]\",raw_text[i:i + seq_length],' raw_text[i + seq_length]',raw_text[i + seq_length],char_to_int[raw_text[i + seq_length]])\n",
        "  seq_in = raw_text[i:i + seq_length]\n",
        "  seq_out = raw_text[i + seq_length]\n",
        "  dataX.append([char_to_int[char] for char in seq_in])\n",
        "  dataY.append(char_to_int[seq_out])\n",
        "  \n",
        "n_patterns = len(dataX) #nombre de patterns / taille de X --> 34\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1)) # on change X tableau --> matrice (34 x 10 x 1)\n",
        "\n",
        "X = X / float(n_vocab) #normaliser -> 0-1\n",
        "y = np_utils.to_categorical(dataY) # tableau 34 --> matrice (34 x 17) de O et 1, exemple : lettre a predire 6 --> 1 en 6eme pos\n",
        "\n",
        "### define the LSTM model ###\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\" #fichier pour sauvegarde\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min') #on sauvegarde\n",
        "callbacks_list = [checkpoint] #liste des sauvegardes\n",
        "\n",
        "### entrainement ###\n",
        "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2257/2257 [==============================] - 29s 12ms/step - loss: 2.9564\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.78788, saving model to weights-improvement-01-2.7879-bigger.hdf5\n",
            "Epoch 2/50\n",
            "2257/2257 [==============================] - 26s 11ms/step - loss: 2.4879\n",
            "\n",
            "Epoch 00002: loss improved from 2.78788 to 2.42756, saving model to weights-improvement-02-2.4276-bigger.hdf5\n",
            "Epoch 3/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 2.2580\n",
            "\n",
            "Epoch 00003: loss improved from 2.42756 to 2.23248, saving model to weights-improvement-03-2.2325-bigger.hdf5\n",
            "Epoch 4/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 2.1289\n",
            "\n",
            "Epoch 00004: loss improved from 2.23248 to 2.10745, saving model to weights-improvement-04-2.1075-bigger.hdf5\n",
            "Epoch 5/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 2.0276\n",
            "\n",
            "Epoch 00005: loss improved from 2.10745 to 2.01343, saving model to weights-improvement-05-2.0134-bigger.hdf5\n",
            "Epoch 6/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.9419\n",
            "\n",
            "Epoch 00006: loss improved from 2.01343 to 1.93883, saving model to weights-improvement-06-1.9388-bigger.hdf5\n",
            "Epoch 7/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.8788\n",
            "\n",
            "Epoch 00007: loss improved from 1.93883 to 1.87966, saving model to weights-improvement-07-1.8797-bigger.hdf5\n",
            "Epoch 8/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.8246\n",
            "\n",
            "Epoch 00008: loss improved from 1.87966 to 1.82706, saving model to weights-improvement-08-1.8271-bigger.hdf5\n",
            "Epoch 9/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.7787\n",
            "\n",
            "Epoch 00009: loss improved from 1.82706 to 1.78211, saving model to weights-improvement-09-1.7821-bigger.hdf5\n",
            "Epoch 10/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.7424\n",
            "\n",
            "Epoch 00010: loss improved from 1.78211 to 1.74399, saving model to weights-improvement-10-1.7440-bigger.hdf5\n",
            "Epoch 11/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.7030\n",
            "\n",
            "Epoch 00011: loss improved from 1.74399 to 1.70894, saving model to weights-improvement-11-1.7089-bigger.hdf5\n",
            "Epoch 12/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.6689\n",
            "\n",
            "Epoch 00012: loss improved from 1.70894 to 1.67705, saving model to weights-improvement-12-1.6770-bigger.hdf5\n",
            "Epoch 13/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.6424\n",
            "\n",
            "Epoch 00013: loss improved from 1.67705 to 1.64965, saving model to weights-improvement-13-1.6496-bigger.hdf5\n",
            "Epoch 14/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.6157\n",
            "\n",
            "Epoch 00014: loss improved from 1.64965 to 1.62353, saving model to weights-improvement-14-1.6235-bigger.hdf5\n",
            "Epoch 15/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.5783\n",
            "\n",
            "Epoch 00015: loss improved from 1.62353 to 1.59509, saving model to weights-improvement-15-1.5951-bigger.hdf5\n",
            "Epoch 16/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.5596\n",
            "\n",
            "Epoch 00016: loss improved from 1.59509 to 1.57094, saving model to weights-improvement-16-1.5709-bigger.hdf5\n",
            "Epoch 17/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.5325\n",
            "\n",
            "Epoch 00017: loss improved from 1.57094 to 1.54972, saving model to weights-improvement-17-1.5497-bigger.hdf5\n",
            "Epoch 18/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.5111\n",
            "\n",
            "Epoch 00018: loss improved from 1.54972 to 1.52915, saving model to weights-improvement-18-1.5291-bigger.hdf5\n",
            "Epoch 19/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.4931\n",
            "\n",
            "Epoch 00019: loss improved from 1.52915 to 1.51022, saving model to weights-improvement-19-1.5102-bigger.hdf5\n",
            "Epoch 20/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.4756\n",
            "\n",
            "Epoch 00020: loss improved from 1.51022 to 1.49239, saving model to weights-improvement-20-1.4924-bigger.hdf5\n",
            "Epoch 21/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.4605\n",
            "\n",
            "Epoch 00021: loss improved from 1.49239 to 1.47466, saving model to weights-improvement-21-1.4747-bigger.hdf5\n",
            "Epoch 22/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.4464\n",
            "\n",
            "Epoch 00022: loss improved from 1.47466 to 1.45826, saving model to weights-improvement-22-1.4583-bigger.hdf5\n",
            "Epoch 23/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.4349\n",
            "\n",
            "Epoch 00023: loss improved from 1.45826 to 1.44063, saving model to weights-improvement-23-1.4406-bigger.hdf5\n",
            "Epoch 24/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.4088\n",
            "\n",
            "Epoch 00024: loss improved from 1.44063 to 1.42529, saving model to weights-improvement-24-1.4253-bigger.hdf5\n",
            "Epoch 25/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.3906\n",
            "\n",
            "Epoch 00025: loss improved from 1.42529 to 1.41046, saving model to weights-improvement-25-1.4105-bigger.hdf5\n",
            "Epoch 26/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.3751\n",
            "\n",
            "Epoch 00026: loss improved from 1.41046 to 1.39640, saving model to weights-improvement-26-1.3964-bigger.hdf5\n",
            "Epoch 27/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.3660\n",
            "\n",
            "Epoch 00027: loss improved from 1.39640 to 1.38634, saving model to weights-improvement-27-1.3863-bigger.hdf5\n",
            "Epoch 28/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.3565\n",
            "\n",
            "Epoch 00028: loss improved from 1.38634 to 1.37105, saving model to weights-improvement-28-1.3711-bigger.hdf5\n",
            "Epoch 29/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.3370\n",
            "\n",
            "Epoch 00029: loss improved from 1.37105 to 1.36154, saving model to weights-improvement-29-1.3615-bigger.hdf5\n",
            "Epoch 30/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.3273\n",
            "\n",
            "Epoch 00030: loss improved from 1.36154 to 1.34495, saving model to weights-improvement-30-1.3449-bigger.hdf5\n",
            "Epoch 31/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.3155\n",
            "\n",
            "Epoch 00031: loss improved from 1.34495 to 1.33759, saving model to weights-improvement-31-1.3376-bigger.hdf5\n",
            "Epoch 32/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.3016\n",
            "\n",
            "Epoch 00032: loss improved from 1.33759 to 1.32768, saving model to weights-improvement-32-1.3277-bigger.hdf5\n",
            "Epoch 33/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.2950\n",
            "\n",
            "Epoch 00033: loss improved from 1.32768 to 1.31448, saving model to weights-improvement-33-1.3145-bigger.hdf5\n",
            "Epoch 34/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.2884\n",
            "\n",
            "Epoch 00034: loss improved from 1.31448 to 1.30707, saving model to weights-improvement-34-1.3071-bigger.hdf5\n",
            "Epoch 35/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.2743\n",
            "\n",
            "Epoch 00035: loss improved from 1.30707 to 1.29641, saving model to weights-improvement-35-1.2964-bigger.hdf5\n",
            "Epoch 36/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.2710\n",
            "\n",
            "Epoch 00036: loss improved from 1.29641 to 1.28500, saving model to weights-improvement-36-1.2850-bigger.hdf5\n",
            "Epoch 37/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.2612\n",
            "\n",
            "Epoch 00037: loss improved from 1.28500 to 1.28139, saving model to weights-improvement-37-1.2814-bigger.hdf5\n",
            "Epoch 38/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.2461\n",
            "\n",
            "Epoch 00038: loss improved from 1.28139 to 1.26755, saving model to weights-improvement-38-1.2676-bigger.hdf5\n",
            "Epoch 39/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.2337\n",
            "\n",
            "Epoch 00039: loss improved from 1.26755 to 1.25854, saving model to weights-improvement-39-1.2585-bigger.hdf5\n",
            "Epoch 40/50\n",
            "2257/2257 [==============================] - 24s 11ms/step - loss: 1.2285\n",
            "\n",
            "Epoch 00040: loss improved from 1.25854 to 1.25170, saving model to weights-improvement-40-1.2517-bigger.hdf5\n",
            "Epoch 41/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.2254\n",
            "\n",
            "Epoch 00041: loss improved from 1.25170 to 1.24444, saving model to weights-improvement-41-1.2444-bigger.hdf5\n",
            "Epoch 42/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.2174\n",
            "\n",
            "Epoch 00042: loss improved from 1.24444 to 1.23695, saving model to weights-improvement-42-1.2370-bigger.hdf5\n",
            "Epoch 43/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.2058\n",
            "\n",
            "Epoch 00043: loss improved from 1.23695 to 1.23058, saving model to weights-improvement-43-1.2306-bigger.hdf5\n",
            "Epoch 44/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.2022\n",
            "\n",
            "Epoch 00044: loss improved from 1.23058 to 1.21893, saving model to weights-improvement-44-1.2189-bigger.hdf5\n",
            "Epoch 45/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.1910\n",
            "\n",
            "Epoch 00045: loss improved from 1.21893 to 1.21674, saving model to weights-improvement-45-1.2167-bigger.hdf5\n",
            "Epoch 46/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.1853\n",
            "\n",
            "Epoch 00046: loss improved from 1.21674 to 1.20616, saving model to weights-improvement-46-1.2062-bigger.hdf5\n",
            "Epoch 47/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.1775\n",
            "\n",
            "Epoch 00047: loss improved from 1.20616 to 1.19545, saving model to weights-improvement-47-1.1954-bigger.hdf5\n",
            "Epoch 48/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.1681\n",
            "\n",
            "Epoch 00048: loss did not improve from 1.19545\n",
            "Epoch 49/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.1660\n",
            "\n",
            "Epoch 00049: loss improved from 1.19545 to 1.18781, saving model to weights-improvement-49-1.1878-bigger.hdf5\n",
            "Epoch 50/50\n",
            "2257/2257 [==============================] - 25s 11ms/step - loss: 1.1636\n",
            "\n",
            "Epoch 00050: loss improved from 1.18781 to 1.18045, saving model to weights-improvement-50-1.1804-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd5df5e9cd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGJIgeyjsr3I"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wAn-tI9cDw7",
        "outputId": "3a0c6663-900d-4e08-86d0-aa801f14aec1"
      },
      "source": [
        "# Load Larger LSTM network and generate text\n",
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "\n",
        "raw_text = raw_text.lower()\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "X = X / float(n_vocab)\n",
        "y = np_utils.to_categorical(dataY)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "filename = \"weights-improvement-50-1.2016-bigger.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# start = numpy.random.randint(0, len(dataX)-1) #on creer un seed un nb alea entre O et le nb_char\n",
        "# pattern = dataX[start] #les 100 caracteres apres la position start\n",
        "# print (\"Seed:\")\n",
        "# print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# print(pattern)\n",
        "seedTexte = \"she came to the room and told me : 'what are you doing here rabbit'. you are in my house ! say somet\"\n",
        "pattern = [char_to_int[value] for value in seedTexte]\n",
        "# print(pattern)\n",
        "\n",
        "\n",
        "# generate characters\n",
        "for i in range(100):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1)) #on reshape l'entrée\n",
        "  x = x / float(n_vocab)#normalise\n",
        "  prediction = model.predict(x, verbose=0) #predire le caractere suivant\n",
        "\n",
        "  index = numpy.argmax(prediction) #index du caractere avec la plus grande proba de prediction\n",
        "  result = int_to_char[index] #entier --> char\n",
        "  sys.stdout.write(result)\n",
        "\n",
        "  seq_in = [int_to_char[value] for value in pattern]#pattern en entiers\n",
        "  pattern.append(index) #on ajoute le nouveau caractere\n",
        "  pattern = pattern[1:len(pattern)]# on eneleve le premier\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hing she was to fond the sabbit say the thme she had never hear her head so his head to hear the tab"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV9NtQKEBfCC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbVF_xBENCXu"
      },
      "source": [
        "# librairies gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors # to load gzip bin files with KeyedVectors.load_word2vec_format\n",
        "from gensim import models\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "from gensim import models\n",
        "model_stanford = models.KeyedVectors.load_word2vec_format(\"glove.6B.100d.w2vformat.txt\", binary=False)\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deStdOpHICUb",
        "outputId": "bca6da54-4852-4c59-c01f-29c43a52d95b"
      },
      "source": [
        "#https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "\n",
        "\n",
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pre-pad sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\treturn in_text\n",
        "\n",
        "def get_embedding_matrix(embeddings,words,vocabulary_size,dimension):\n",
        "  # Preparation de la matrice des embeddings embedding matrix\n",
        "  embedding_matrix = np.zeros((vocabulary_size, dimension)) #(22 x 100)\n",
        "  for word, i in words.word_index.items():\n",
        "    try: # lever l'exception si un mot n'est pas trouvé dans les embeddings\n",
        "        embedding_vector = embeddings.wv[word] \n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError: # mot non trouvé dans les embeddings\n",
        "        # print(word)\n",
        "        pass\n",
        "  return embedding_matrix\n",
        "\n",
        "\n",
        "# source text\n",
        "filename = \"wonderland.txt\"\n",
        "data = open(filename, 'r', encoding='utf-8').read()\n",
        "# data = \"\"\" Jack and Jill went up the hill\\n\n",
        "# \t\tTo fetch a pail of water\\n\n",
        "# \t\tJack fell down and broke his crown\\n\n",
        "# \t\tAnd Jill came tumbling after\\n \"\"\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded = tokenizer.texts_to_sequences([data])[0] #codage des mots\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1 #pourquoi +1 ???\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "# encode 2 words -> 1 word\n",
        "sequences = list()\n",
        "for i in range(2, len(encoded)):\n",
        "\tsequence = encoded[i-2:i+1]\n",
        "\tsequences.append(sequence)\n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(seq) for seq in sequences]) #taille de la sequence la plus longue ? 3\n",
        "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre') #padding\n",
        "\n",
        "# split into input and output elements\n",
        "sequences = array(sequences) #utile??\n",
        "X, y = sequences[:,:-1],sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "DIMENSION=100 #max_length#20\n",
        "embedding_matrix=get_embedding_matrix(model_stanford,tokenizer,vocab_size,DIMENSION)\n",
        "\n",
        "#---------- model ---------------\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length-1))#, trainable=False)) \n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# print(model.summary())\n",
        "model.fit(X, y, epochs=100, verbose=2)\n",
        "model.save('model_embeddings.bin')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 2862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "869/869 - 9s - loss: 6.2542 - accuracy: 0.0614\n",
            "Epoch 2/100\n",
            "869/869 - 7s - loss: 5.7452 - accuracy: 0.0892\n",
            "Epoch 3/100\n",
            "869/869 - 7s - loss: 5.4161 - accuracy: 0.1161\n",
            "Epoch 4/100\n",
            "869/869 - 7s - loss: 5.1370 - accuracy: 0.1382\n",
            "Epoch 5/100\n",
            "869/869 - 7s - loss: 4.9112 - accuracy: 0.1549\n",
            "Epoch 6/100\n",
            "869/869 - 7s - loss: 4.7209 - accuracy: 0.1674\n",
            "Epoch 7/100\n",
            "869/869 - 7s - loss: 4.5523 - accuracy: 0.1770\n",
            "Epoch 8/100\n",
            "869/869 - 7s - loss: 4.4007 - accuracy: 0.1904\n",
            "Epoch 9/100\n",
            "869/869 - 7s - loss: 4.2632 - accuracy: 0.2009\n",
            "Epoch 10/100\n",
            "869/869 - 7s - loss: 4.1371 - accuracy: 0.2103\n",
            "Epoch 11/100\n",
            "869/869 - 7s - loss: 4.0166 - accuracy: 0.2190\n",
            "Epoch 12/100\n",
            "869/869 - 7s - loss: 3.9044 - accuracy: 0.2291\n",
            "Epoch 13/100\n",
            "869/869 - 7s - loss: 3.7978 - accuracy: 0.2403\n",
            "Epoch 14/100\n",
            "869/869 - 7s - loss: 3.6947 - accuracy: 0.2515\n",
            "Epoch 15/100\n",
            "869/869 - 7s - loss: 3.5972 - accuracy: 0.2634\n",
            "Epoch 16/100\n",
            "869/869 - 7s - loss: 3.5037 - accuracy: 0.2767\n",
            "Epoch 17/100\n",
            "869/869 - 7s - loss: 3.4149 - accuracy: 0.2892\n",
            "Epoch 18/100\n",
            "869/869 - 7s - loss: 3.3298 - accuracy: 0.3012\n",
            "Epoch 19/100\n",
            "869/869 - 7s - loss: 3.2495 - accuracy: 0.3141\n",
            "Epoch 20/100\n",
            "869/869 - 7s - loss: 3.1715 - accuracy: 0.3239\n",
            "Epoch 21/100\n",
            "869/869 - 7s - loss: 3.0991 - accuracy: 0.3365\n",
            "Epoch 22/100\n",
            "869/869 - 7s - loss: 3.0293 - accuracy: 0.3484\n",
            "Epoch 23/100\n",
            "869/869 - 7s - loss: 2.9635 - accuracy: 0.3581\n",
            "Epoch 24/100\n",
            "869/869 - 7s - loss: 2.9009 - accuracy: 0.3679\n",
            "Epoch 25/100\n",
            "869/869 - 7s - loss: 2.8423 - accuracy: 0.3783\n",
            "Epoch 26/100\n",
            "869/869 - 7s - loss: 2.7874 - accuracy: 0.3852\n",
            "Epoch 27/100\n",
            "869/869 - 7s - loss: 2.7346 - accuracy: 0.3924\n",
            "Epoch 28/100\n",
            "869/869 - 7s - loss: 2.6857 - accuracy: 0.4000\n",
            "Epoch 29/100\n",
            "869/869 - 7s - loss: 2.6390 - accuracy: 0.4074\n",
            "Epoch 30/100\n",
            "869/869 - 7s - loss: 2.5933 - accuracy: 0.4139\n",
            "Epoch 31/100\n",
            "869/869 - 7s - loss: 2.5513 - accuracy: 0.4225\n",
            "Epoch 32/100\n",
            "869/869 - 7s - loss: 2.5097 - accuracy: 0.4272\n",
            "Epoch 33/100\n",
            "869/869 - 7s - loss: 2.4707 - accuracy: 0.4359\n",
            "Epoch 34/100\n",
            "869/869 - 7s - loss: 2.4346 - accuracy: 0.4403\n",
            "Epoch 35/100\n",
            "869/869 - 7s - loss: 2.3989 - accuracy: 0.4459\n",
            "Epoch 36/100\n",
            "869/869 - 7s - loss: 2.3657 - accuracy: 0.4488\n",
            "Epoch 37/100\n",
            "869/869 - 7s - loss: 2.3329 - accuracy: 0.4538\n",
            "Epoch 38/100\n",
            "869/869 - 7s - loss: 2.3024 - accuracy: 0.4594\n",
            "Epoch 39/100\n",
            "869/869 - 7s - loss: 2.2716 - accuracy: 0.4647\n",
            "Epoch 40/100\n",
            "869/869 - 7s - loss: 2.2444 - accuracy: 0.4690\n",
            "Epoch 41/100\n",
            "869/869 - 7s - loss: 2.2164 - accuracy: 0.4735\n",
            "Epoch 42/100\n",
            "869/869 - 7s - loss: 2.1911 - accuracy: 0.4778\n",
            "Epoch 43/100\n",
            "869/869 - 7s - loss: 2.1650 - accuracy: 0.4812\n",
            "Epoch 44/100\n",
            "869/869 - 7s - loss: 2.1413 - accuracy: 0.4839\n",
            "Epoch 45/100\n",
            "869/869 - 7s - loss: 2.1192 - accuracy: 0.4868\n",
            "Epoch 46/100\n",
            "869/869 - 7s - loss: 2.0953 - accuracy: 0.4904\n",
            "Epoch 47/100\n",
            "869/869 - 7s - loss: 2.0747 - accuracy: 0.4936\n",
            "Epoch 48/100\n",
            "869/869 - 7s - loss: 2.0547 - accuracy: 0.4977\n",
            "Epoch 49/100\n",
            "869/869 - 7s - loss: 2.0335 - accuracy: 0.5003\n",
            "Epoch 50/100\n",
            "869/869 - 7s - loss: 2.0153 - accuracy: 0.5030\n",
            "Epoch 51/100\n",
            "869/869 - 7s - loss: 1.9962 - accuracy: 0.5063\n",
            "Epoch 52/100\n",
            "869/869 - 7s - loss: 1.9784 - accuracy: 0.5080\n",
            "Epoch 53/100\n",
            "869/869 - 7s - loss: 1.9608 - accuracy: 0.5103\n",
            "Epoch 54/100\n",
            "869/869 - 7s - loss: 1.9455 - accuracy: 0.5128\n",
            "Epoch 55/100\n",
            "869/869 - 7s - loss: 1.9281 - accuracy: 0.5148\n",
            "Epoch 56/100\n",
            "869/869 - 7s - loss: 1.9133 - accuracy: 0.5160\n",
            "Epoch 57/100\n",
            "869/869 - 7s - loss: 1.8977 - accuracy: 0.5214\n",
            "Epoch 58/100\n",
            "869/869 - 7s - loss: 1.8835 - accuracy: 0.5211\n",
            "Epoch 59/100\n",
            "869/869 - 7s - loss: 1.8678 - accuracy: 0.5234\n",
            "Epoch 60/100\n",
            "869/869 - 7s - loss: 1.8557 - accuracy: 0.5258\n",
            "Epoch 61/100\n",
            "869/869 - 7s - loss: 1.8411 - accuracy: 0.5287\n",
            "Epoch 62/100\n",
            "869/869 - 7s - loss: 1.8286 - accuracy: 0.5292\n",
            "Epoch 63/100\n",
            "869/869 - 7s - loss: 1.8164 - accuracy: 0.5312\n",
            "Epoch 64/100\n",
            "869/869 - 7s - loss: 1.8036 - accuracy: 0.5307\n",
            "Epoch 65/100\n",
            "869/869 - 7s - loss: 1.7932 - accuracy: 0.5324\n",
            "Epoch 66/100\n",
            "869/869 - 7s - loss: 1.7811 - accuracy: 0.5342\n",
            "Epoch 67/100\n",
            "869/869 - 7s - loss: 1.7696 - accuracy: 0.5368\n",
            "Epoch 68/100\n",
            "869/869 - 7s - loss: 1.7586 - accuracy: 0.5373\n",
            "Epoch 69/100\n",
            "869/869 - 7s - loss: 1.7486 - accuracy: 0.5398\n",
            "Epoch 70/100\n",
            "869/869 - 7s - loss: 1.7383 - accuracy: 0.5400\n",
            "Epoch 71/100\n",
            "869/869 - 7s - loss: 1.7281 - accuracy: 0.5424\n",
            "Epoch 72/100\n",
            "869/869 - 7s - loss: 1.7181 - accuracy: 0.5422\n",
            "Epoch 73/100\n",
            "869/869 - 7s - loss: 1.7087 - accuracy: 0.5441\n",
            "Epoch 74/100\n",
            "869/869 - 7s - loss: 1.7003 - accuracy: 0.5434\n",
            "Epoch 75/100\n",
            "869/869 - 7s - loss: 1.6901 - accuracy: 0.5452\n",
            "Epoch 76/100\n",
            "869/869 - 7s - loss: 1.6811 - accuracy: 0.5462\n",
            "Epoch 77/100\n",
            "869/869 - 7s - loss: 1.6727 - accuracy: 0.5494\n",
            "Epoch 78/100\n",
            "869/869 - 7s - loss: 1.6651 - accuracy: 0.5498\n",
            "Epoch 79/100\n",
            "869/869 - 7s - loss: 1.6557 - accuracy: 0.5515\n",
            "Epoch 80/100\n",
            "869/869 - 7s - loss: 1.6497 - accuracy: 0.5507\n",
            "Epoch 81/100\n",
            "869/869 - 7s - loss: 1.6399 - accuracy: 0.5513\n",
            "Epoch 82/100\n",
            "869/869 - 7s - loss: 1.6324 - accuracy: 0.5548\n",
            "Epoch 83/100\n",
            "869/869 - 7s - loss: 1.6256 - accuracy: 0.5535\n",
            "Epoch 84/100\n",
            "869/869 - 7s - loss: 1.6186 - accuracy: 0.5534\n",
            "Epoch 85/100\n",
            "869/869 - 7s - loss: 1.6107 - accuracy: 0.5551\n",
            "Epoch 86/100\n",
            "869/869 - 7s - loss: 1.6042 - accuracy: 0.5565\n",
            "Epoch 87/100\n",
            "869/869 - 7s - loss: 1.5966 - accuracy: 0.5588\n",
            "Epoch 88/100\n",
            "869/869 - 7s - loss: 1.5916 - accuracy: 0.5573\n",
            "Epoch 89/100\n",
            "869/869 - 7s - loss: 1.5837 - accuracy: 0.5578\n",
            "Epoch 90/100\n",
            "869/869 - 7s - loss: 1.5789 - accuracy: 0.5602\n",
            "Epoch 91/100\n",
            "869/869 - 7s - loss: 1.5711 - accuracy: 0.5590\n",
            "Epoch 92/100\n",
            "869/869 - 7s - loss: 1.5653 - accuracy: 0.5599\n",
            "Epoch 93/100\n",
            "869/869 - 7s - loss: 1.5606 - accuracy: 0.5608\n",
            "Epoch 94/100\n",
            "869/869 - 7s - loss: 1.5534 - accuracy: 0.5614\n",
            "Epoch 95/100\n",
            "869/869 - 7s - loss: 1.5473 - accuracy: 0.5617\n",
            "Epoch 96/100\n",
            "869/869 - 7s - loss: 1.5413 - accuracy: 0.5646\n",
            "Epoch 97/100\n",
            "869/869 - 7s - loss: 1.5373 - accuracy: 0.5639\n",
            "Epoch 98/100\n",
            "869/869 - 7s - loss: 1.5308 - accuracy: 0.5653\n",
            "Epoch 99/100\n",
            "869/869 - 7s - loss: 1.5268 - accuracy: 0.5649\n",
            "Epoch 100/100\n",
            "869/869 - 7s - loss: 1.5204 - accuracy: 0.5649\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_42_layer_call_and_return_conditional_losses, lstm_cell_42_layer_call_fn, lstm_cell_42_layer_call_fn, lstm_cell_42_layer_call_and_return_conditional_losses, lstm_cell_42_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_42_layer_call_and_return_conditional_losses, lstm_cell_42_layer_call_fn, lstm_cell_42_layer_call_fn, lstm_cell_42_layer_call_and_return_conditional_losses, lstm_cell_42_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model_embeddings.bin/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model_embeddings.bin/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDJdOeAl6afV",
        "outputId": "abb8b05f-9a63-490c-c7cc-e7a3cd95ed60"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "new_model = keras.models.load_model('model_embeddings.bin')\n",
        "\n",
        "print(generate_seq(new_model, tokenizer, max_length-1, 'so long', 10))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "so long since she had not gone much farther before she had\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}