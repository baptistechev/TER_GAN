{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMGeneratingOpinion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePKb-MvOb4HR",
        "outputId": "95a4b8a2-8d03-463d-b847-4d0307ecc083"
      },
      "source": [
        "# pour monter son drive Google Drive local\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My Drive/Colab Notebooks\n",
        "%ls\n",
        "# pour une utilisation sur une machine locale changer le chemin ci-dessous et décommenter la ligne\n",
        "#%cd ./"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Colab Notebooks\n",
            " 1_-_Classification_de_donnees.ipynb\n",
            " \u001b[0m\u001b[01;34m3_ClassDonneeText\u001b[0m/\n",
            " 3_-_Classification_de_donnees_textuelles.ipynb\n",
            " 6_-_Les_word_embeddings_solutionsintegrees.ipynb\n",
            " amazon_cells_labelled.txt\n",
            " LSTMForGeneratingText.ipynb\n",
            " LSTMForGeneratingText_VersioCommente.ipynb\n",
            " LSTMForGeneratingText-VersioCommente.ipynb\n",
            " SPOILER_4_-_Utiliser_un_modele_solutionsintegrees.ipynb\n",
            " Untitled\n",
            "'Untitled (1)'\n",
            "'Untitled (2)'\n",
            "'Untitled (3)'\n",
            " weights-improvement-50-1.1972-bigger.hdf5\n",
            " wonderland.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05aWFL9ld8NW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4782239-ecbe-4c94-be2c-6eb2ce4765fd"
      },
      "source": [
        "#https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
        "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import pandas as pd\n",
        "# load ascii text and covert to lowercase\n",
        "\n",
        "#charge fichier\n",
        "# filename = \"ReviewsLabelled.csv\"\n",
        "# raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "\n",
        "df = pd.read_csv(\"ReviewsLabelled.csv\", names=['sentence','sentiment','source'], header=0,sep='\\t', encoding='utf8')\n",
        "\n",
        "object_dataframe = df[df['sentiment'] >= 1].sentence\n",
        "raw_text = str(object_dataframe)\n",
        "### pre-traitement du texte ###\n",
        "\n",
        "raw_text = raw_text.lower() #texte en minuscule\n",
        "chars = sorted(list(set(raw_text))) # creation dictionnaire\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) # code du dico en entiers\n",
        "\n",
        "n_chars = len(raw_text) #nb caractere du texte\n",
        "n_vocab = len(chars) #nb de lettres distincte\n",
        "\n",
        "# print (\"Tous les characteres: \",chars)\n",
        "# print (\"Total Characters: \", n_chars)\n",
        "# print (\"Total Vocab: \", n_vocab)\n",
        "\n",
        "### Creation des matrices entré / sortie ###\n",
        "seq_length = 100 #100 pour Alice\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "  # print (n_chars - seq_length,' ',i,\"raw_text[i:i + seq_length]\",raw_text[i:i + seq_length],' raw_text[i + seq_length]',raw_text[i + seq_length],char_to_int[raw_text[i + seq_length]])\n",
        "  seq_in = raw_text[i:i + seq_length]\n",
        "  seq_out = raw_text[i + seq_length]\n",
        "  dataX.append([char_to_int[char] for char in seq_in])\n",
        "  dataY.append(char_to_int[seq_out])\n",
        "  \n",
        "n_patterns = len(dataX) #nombre de patterns / taille de X --> 34\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1)) # on change X tableau --> matrice (34 x 10 x 1)\n",
        "\n",
        "X = X / float(n_vocab) #normaliser -> 0-1\n",
        "y = np_utils.to_categorical(dataY) # tableau 34 --> matrice (34 x 17) de O et 1, exemple : lettre a predire 6 --> 1 en 6eme pos\n",
        "\n",
        "### define the LSTM model ###\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\" #fichier pour sauvegarde\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min') #on sauvegarde\n",
        "callbacks_list = [checkpoint] #liste des sauvegardes\n",
        "\n",
        "### entrainement ###\n",
        "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "10/10 [==============================] - 4s 74ms/step - loss: 3.3888\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.11801, saving model to weights-improvement-01-3.1180-bigger.hdf5\n",
            "Epoch 2/50\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 2.6688\n",
            "\n",
            "Epoch 00002: loss improved from 3.11801 to 2.71368, saving model to weights-improvement-02-2.7137-bigger.hdf5\n",
            "Epoch 3/50\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 2.6920\n",
            "\n",
            "Epoch 00003: loss improved from 2.71368 to 2.66782, saving model to weights-improvement-03-2.6678-bigger.hdf5\n",
            "Epoch 4/50\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 2.6636\n",
            "\n",
            "Epoch 00004: loss improved from 2.66782 to 2.61061, saving model to weights-improvement-04-2.6106-bigger.hdf5\n",
            "Epoch 5/50\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 2.5986\n",
            "\n",
            "Epoch 00005: loss improved from 2.61061 to 2.59028, saving model to weights-improvement-05-2.5903-bigger.hdf5\n",
            "Epoch 6/50\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 2.5380\n",
            "\n",
            "Epoch 00006: loss improved from 2.59028 to 2.50942, saving model to weights-improvement-06-2.5094-bigger.hdf5\n",
            "Epoch 7/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 2.5975\n",
            "\n",
            "Epoch 00007: loss did not improve from 2.50942\n",
            "Epoch 8/50\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 2.5351\n",
            "\n",
            "Epoch 00008: loss improved from 2.50942 to 2.48181, saving model to weights-improvement-08-2.4818-bigger.hdf5\n",
            "Epoch 9/50\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 2.4104\n",
            "\n",
            "Epoch 00009: loss improved from 2.48181 to 2.46546, saving model to weights-improvement-09-2.4655-bigger.hdf5\n",
            "Epoch 10/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 2.4827\n",
            "\n",
            "Epoch 00010: loss did not improve from 2.46546\n",
            "Epoch 11/50\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 2.5611\n",
            "\n",
            "Epoch 00011: loss improved from 2.46546 to 2.46274, saving model to weights-improvement-11-2.4627-bigger.hdf5\n",
            "Epoch 12/50\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 2.3615\n",
            "\n",
            "Epoch 00012: loss improved from 2.46274 to 2.44545, saving model to weights-improvement-12-2.4455-bigger.hdf5\n",
            "Epoch 13/50\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 2.5201\n",
            "\n",
            "Epoch 00013: loss improved from 2.44545 to 2.43403, saving model to weights-improvement-13-2.4340-bigger.hdf5\n",
            "Epoch 14/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 2.4186\n",
            "\n",
            "Epoch 00014: loss did not improve from 2.43403\n",
            "Epoch 15/50\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 2.4736\n",
            "\n",
            "Epoch 00015: loss did not improve from 2.43403\n",
            "Epoch 16/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 2.3729\n",
            "\n",
            "Epoch 00016: loss improved from 2.43403 to 2.43380, saving model to weights-improvement-16-2.4338-bigger.hdf5\n",
            "Epoch 17/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 2.4054\n",
            "\n",
            "Epoch 00017: loss improved from 2.43380 to 2.42560, saving model to weights-improvement-17-2.4256-bigger.hdf5\n",
            "Epoch 18/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 2.3932\n",
            "\n",
            "Epoch 00018: loss improved from 2.42560 to 2.40054, saving model to weights-improvement-18-2.4005-bigger.hdf5\n",
            "Epoch 19/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 2.4500\n",
            "\n",
            "Epoch 00019: loss improved from 2.40054 to 2.39479, saving model to weights-improvement-19-2.3948-bigger.hdf5\n",
            "Epoch 20/50\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 2.2961\n",
            "\n",
            "Epoch 00020: loss improved from 2.39479 to 2.37666, saving model to weights-improvement-20-2.3767-bigger.hdf5\n",
            "Epoch 21/50\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 2.3223\n",
            "\n",
            "Epoch 00021: loss did not improve from 2.37666\n",
            "Epoch 22/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 2.3731\n",
            "\n",
            "Epoch 00022: loss improved from 2.37666 to 2.35175, saving model to weights-improvement-22-2.3517-bigger.hdf5\n",
            "Epoch 23/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 2.3313\n",
            "\n",
            "Epoch 00023: loss did not improve from 2.35175\n",
            "Epoch 24/50\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 2.3478\n",
            "\n",
            "Epoch 00024: loss improved from 2.35175 to 2.32944, saving model to weights-improvement-24-2.3294-bigger.hdf5\n",
            "Epoch 25/50\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 2.3996\n",
            "\n",
            "Epoch 00025: loss improved from 2.32944 to 2.30843, saving model to weights-improvement-25-2.3084-bigger.hdf5\n",
            "Epoch 26/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 2.3975\n",
            "\n",
            "Epoch 00026: loss did not improve from 2.30843\n",
            "Epoch 27/50\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 2.4303\n",
            "\n",
            "Epoch 00027: loss improved from 2.30843 to 2.30461, saving model to weights-improvement-27-2.3046-bigger.hdf5\n",
            "Epoch 28/50\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 2.4973\n",
            "\n",
            "Epoch 00028: loss did not improve from 2.30461\n",
            "Epoch 29/50\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 2.3424\n",
            "\n",
            "Epoch 00029: loss did not improve from 2.30461\n",
            "Epoch 30/50\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 2.2988\n",
            "\n",
            "Epoch 00030: loss improved from 2.30461 to 2.29772, saving model to weights-improvement-30-2.2977-bigger.hdf5\n",
            "Epoch 31/50\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 2.2796\n",
            "\n",
            "Epoch 00031: loss improved from 2.29772 to 2.27942, saving model to weights-improvement-31-2.2794-bigger.hdf5\n",
            "Epoch 32/50\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 2.2361\n",
            "\n",
            "Epoch 00032: loss improved from 2.27942 to 2.25254, saving model to weights-improvement-32-2.2525-bigger.hdf5\n",
            "Epoch 33/50\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 2.2595\n",
            "\n",
            "Epoch 00033: loss did not improve from 2.25254\n",
            "Epoch 34/50\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 2.2680\n",
            "\n",
            "Epoch 00034: loss improved from 2.25254 to 2.22990, saving model to weights-improvement-34-2.2299-bigger.hdf5\n",
            "Epoch 35/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 2.2647\n",
            "\n",
            "Epoch 00035: loss improved from 2.22990 to 2.21431, saving model to weights-improvement-35-2.2143-bigger.hdf5\n",
            "Epoch 36/50\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 2.1574\n",
            "\n",
            "Epoch 00036: loss improved from 2.21431 to 2.18423, saving model to weights-improvement-36-2.1842-bigger.hdf5\n",
            "Epoch 37/50\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 2.2299\n",
            "\n",
            "Epoch 00037: loss did not improve from 2.18423\n",
            "Epoch 38/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 2.2449\n",
            "\n",
            "Epoch 00038: loss did not improve from 2.18423\n",
            "Epoch 39/50\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 2.1656\n",
            "\n",
            "Epoch 00039: loss improved from 2.18423 to 2.16012, saving model to weights-improvement-39-2.1601-bigger.hdf5\n",
            "Epoch 40/50\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 2.1820\n",
            "\n",
            "Epoch 00040: loss improved from 2.16012 to 2.13539, saving model to weights-improvement-40-2.1354-bigger.hdf5\n",
            "Epoch 41/50\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 2.1757\n",
            "\n",
            "Epoch 00041: loss improved from 2.13539 to 2.12020, saving model to weights-improvement-41-2.1202-bigger.hdf5\n",
            "Epoch 42/50\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 2.1482\n",
            "\n",
            "Epoch 00042: loss improved from 2.12020 to 2.10652, saving model to weights-improvement-42-2.1065-bigger.hdf5\n",
            "Epoch 43/50\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 2.0579\n",
            "\n",
            "Epoch 00043: loss improved from 2.10652 to 2.06125, saving model to weights-improvement-43-2.0612-bigger.hdf5\n",
            "Epoch 44/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 1.9892\n",
            "\n",
            "Epoch 00044: loss did not improve from 2.06125\n",
            "Epoch 45/50\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 2.0614\n",
            "\n",
            "Epoch 00045: loss improved from 2.06125 to 2.02762, saving model to weights-improvement-45-2.0276-bigger.hdf5\n",
            "Epoch 46/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 2.0033\n",
            "\n",
            "Epoch 00046: loss improved from 2.02762 to 2.01728, saving model to weights-improvement-46-2.0173-bigger.hdf5\n",
            "Epoch 47/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 1.9050\n",
            "\n",
            "Epoch 00047: loss improved from 2.01728 to 1.98320, saving model to weights-improvement-47-1.9832-bigger.hdf5\n",
            "Epoch 48/50\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 1.9663\n",
            "\n",
            "Epoch 00048: loss did not improve from 1.98320\n",
            "Epoch 49/50\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 2.0110\n",
            "\n",
            "Epoch 00049: loss did not improve from 1.98320\n",
            "Epoch 50/50\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 2.0217\n",
            "\n",
            "Epoch 00050: loss did not improve from 1.98320\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc999f6bc90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGJIgeyjsr3I"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wAn-tI9cDw7",
        "outputId": "3e06baad-b957-49e8-b937-9c01ba2c6708"
      },
      "source": [
        "# Load Larger LSTM network and generate text\n",
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "df = pd.read_csv(\"ReviewsLabelled.csv\", names=['sentence','sentiment','source'], header=0,sep='\\t', encoding='utf8')\n",
        "object_dataframe = df[df['sentiment'] >= 1].sentence\n",
        "raw_text = str(object_dataframe)\n",
        "\n",
        "# print(raw_text)\n",
        "\n",
        "raw_text = raw_text.lower()\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "X = X / float(n_vocab)\n",
        "y = np_utils.to_categorical(dataY)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "filename = \"weights-improvement-47-1.9832-bigger.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# start = numpy.random.randint(0, len(dataX)-1) #on creer un seed un nb alea entre O et le nb_char\n",
        "# pattern = dataX[start] #les 100 caracteres apres la position start\n",
        "# print (\"Seed:\")\n",
        "# print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# print(pattern)\n",
        "\n",
        "seedTexte = \"i bought this to use with my kindle fire and absolutely loved it. this product is ideal for people a\"\n",
        "pattern = [char_to_int[value] for value in seedTexte]\n",
        "# print(pattern)\n",
        "\n",
        "\n",
        "# generate characters\n",
        "for i in range(10):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1)) #on reshape l'entrée\n",
        "  x = x / float(n_vocab)#normalise\n",
        "  prediction = model.predict(x, verbose=0) #predire le caractere suivant\n",
        "\n",
        "  index = numpy.argmax(prediction) #index du caractere avec la plus grande proba de prediction\n",
        "  result = int_to_char[index] #entier --> char\n",
        "  sys.stdout.write(result)\n",
        "\n",
        "  seq_in = [int_to_char[value] for value in pattern]#pattern en entiers\n",
        "  pattern.append(index) #on ajoute le nouveau caractere\n",
        "  pattern = pattern[1:len(pattern)]# on eneleve le premier\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!!!!!!!!!."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV9NtQKEBfCC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbVF_xBENCXu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "b75eeab1-8573-450b-fb67-0b93b68401f7"
      },
      "source": [
        "# librairies gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors # to load gzip bin files with KeyedVectors.load_word2vec_format\n",
        "from gensim import models\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "from gensim import models\n",
        "model_stanford = models.KeyedVectors.load_word2vec_format(\"glove.6B.100d.w2vformat.txt\", binary=False)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-0ca48ffeb822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel_stanford\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove.6B.100d.w2vformat.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.w2vformat.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "deStdOpHICUb",
        "outputId": "9a5e744a-5542-40fa-a40e-2616f482a5d0"
      },
      "source": [
        "#https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "\n",
        "\n",
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pre-pad sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\treturn in_text\n",
        "\n",
        "def get_embedding_matrix(embeddings,words,vocabulary_size,dimension):\n",
        "  # Preparation de la matrice des embeddings embedding matrix\n",
        "  embedding_matrix = np.zeros((vocabulary_size, dimension)) #(22 x 100)\n",
        "  for word, i in words.word_index.items():\n",
        "    try: # lever l'exception si un mot n'est pas trouvé dans les embeddings\n",
        "        embedding_vector = embeddings.wv[word] \n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError: # mot non trouvé dans les embeddings\n",
        "        # print(word)\n",
        "        pass\n",
        "  return embedding_matrix\n",
        "\n",
        "\n",
        "# source text\n",
        "filename = \"wonderland.txt\"\n",
        "data = open(filename, 'r', encoding='utf-8').read()\n",
        "# data = \"\"\" Jack and Jill went up the hill\\n\n",
        "# \t\tTo fetch a pail of water\\n\n",
        "# \t\tJack fell down and broke his crown\\n\n",
        "# \t\tAnd Jill came tumbling after\\n \"\"\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded = tokenizer.texts_to_sequences([data])[0] #codage des mots\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1 #pourquoi +1 ???\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "# encode 2 words -> 1 word\n",
        "sequences = list()\n",
        "for i in range(2, len(encoded)):\n",
        "\tsequence = encoded[i-2:i+1]\n",
        "\tsequences.append(sequence)\n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(seq) for seq in sequences]) #taille de la sequence la plus longue ? 3\n",
        "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre') #padding\n",
        "\n",
        "# split into input and output elements\n",
        "sequences = array(sequences) #utile??\n",
        "X, y = sequences[:,:-1],sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "DIMENSION=100 #max_length#20\n",
        "embedding_matrix=get_embedding_matrix(model_stanford,tokenizer,vocab_size,DIMENSION)\n",
        "\n",
        "#---------- model ---------------\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length-1))#, trainable=False)) \n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# print(model.summary())\n",
        "model.fit(X, y, epochs=100, verbose=2)\n",
        "model.save('model_embeddings.bin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 2862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7046bad9eedf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mDIMENSION\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;31m#max_length#20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_embedding_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_stanford\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDIMENSION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#---------- model ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_stanford' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDJdOeAl6afV",
        "outputId": "abb8b05f-9a63-490c-c7cc-e7a3cd95ed60"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "new_model = keras.models.load_model('model_embeddings.bin')\n",
        "\n",
        "print(generate_seq(new_model, tokenizer, max_length-1, 'so long', 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "so long since she had not gone much farther before she had\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}